# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hATu28uE6FastklJVGRWyFqp70h23X6F
"""

import pandas as pd
import torch
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

# Define a consistent model path
model_path = '/content/pruned_model/Phi-2/0.4'  # Replace with your actual model path

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path).to('cuda')

# Ensure padding token is set
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})
model.config.pad_token_id = tokenizer.pad_token_id

print(f"Model loaded successfully from {model_path}.")

# Function to evaluate metrics for CommonsenseQA
def evaluate_metrics_commonsenseqa(model, tokenizer, dataset):
    y_true = []
    y_pred = []

    for _, item in dataset.iterrows():
        question = item['question']
        choices = item['choices']['text']
        correct_answer = item['answerKey']

        if not correct_answer:
            continue

        scores = []
        for choice in choices:
            input_text = f"{question} {choice}"
            inputs = tokenizer(input_text, return_tensors="pt").to('cuda')
            with torch.no_grad():
                outputs = model(**inputs)
            logits = outputs.logits[:, :-1, :]
            target_ids = inputs['input_ids'][:, 1:]
            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
            loss = loss_fct(logits.permute(0, 2, 1), target_ids)
            scores.append(-loss.sum().item())

        predicted_index = torch.argmax(torch.tensor(scores)).item()
        y_true.append(ord(correct_answer) - ord('A'))
        y_pred.append(predicted_index)

    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')

    return accuracy, precision, recall, f1

# Evaluate on the validation set (CommonsenseQA)
validation_data = pd.read_parquet('/content/validation_commonsenseqa.parquet')
accuracy, precision, recall, f1 = evaluate_metrics_commonsenseqa(model, tokenizer, validation_data)
print(f"CommonsenseQA Evaluation Metrics:")
print(f"Validation Accuracy: {accuracy * 100:.2f}%")
print(f"Validation Precision: {precision * 100:.2f}%")
print(f"Validation Recall: {recall * 100:.2f}%")
print(f"Validation F1-Score: {f1 * 100:.2f}%")

# Function to load and prepare dataset from CSV
def prepare_dataset(dataframe):
    return [
        {
            'question': row['question'],
            'answer0': row['answer0'],
            'answer1': row['answer1'],
            'answer2': row['answer2'],
            'answer3': row['answer3'],
            'label': row['label']
        }
        for _, row in dataframe.iterrows()
    ]

# Evaluation function for pruned model on CosmosQA
def evaluate_cosmosqa_pruned(model, tokenizer, dataset):
    y_true = []
    y_pred = []

    for item in dataset:
        question = item['question']
        choices = [item['answer0'], item['answer1'], item['answer2'], item['answer3']]
        correct_answer = item['label']

        scores = []
        for choice in choices:
            input_text = f"{question} {choice}"
            inputs = tokenizer(input_text, return_tensors="pt").to('cuda')
            with torch.no_grad():
                outputs = model(**inputs)
            logits = outputs.logits[:, :-1, :]
            target_ids = inputs['input_ids'][:, 1:]
            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
            loss = loss_fct(logits.permute(0, 2, 1), target_ids)
            scores.append(-loss.sum().item())

        predicted_index = torch.argmax(torch.tensor(scores)).item()
        y_true.append(correct_answer)
        y_pred.append(predicted_index)

    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')

    return accuracy, precision, recall, f1

# Load a CSV validation dataset
csv_file_path = '/content/validation_cosmosqa.csv'
validation_data = pd.read_csv(csv_file_path)
validation_dataset = prepare_dataset(validation_data)
print(f"Validation set size: {len(validation_data)}")

# Evaluate the model on CosmosQA dataset
accuracy, precision, recall, f1 = evaluate_cosmosqa_pruned(model, tokenizer, validation_dataset)
print(f"CosmosQA Evaluation Metrics:")
print(f"Validation Accuracy: {accuracy * 100:.2f}%")
print(f"Validation Precision: {precision * 100:.2f}%")
print(f"Validation Recall: {recall * 100:.2f}%")
print(f"Validation F1-Score: {f1 * 100:.2f}%")

# Function to load LogiQA data from a text file
def load_logiqa_data(file_path):
    with open(file_path, 'r') as file:
        lines = file.readlines()

    dataset = []
    i = 0
    while i < len(lines):
        if lines[i].strip() == '':
            i += 1  # Skip the blank line
            continue

        correct_choice = lines[i].strip()
        context = lines[i + 1].strip()
        question = lines[i + 2].strip()
        choices = [lines[i + 3].strip(), lines[i + 4].strip(), lines[i + 5].strip(), lines[i + 6].strip()]

        dataset.append({
            'context': context,
            'question': question,
            'choices': choices,
            'correct_choice': correct_choice
        })

        i += 7  # Move to the next block

    return dataset

# Load the LogiQA dataset
file_path = '/content/validation_logiqa.txt'
logiqa_data = load_logiqa_data(file_path)
print(f"Loaded {len(logiqa_data)} examples from LogiQA dataset.")

# Evaluation function for the LogiQA dataset
def evaluate_logiqa_pruned(model, tokenizer, dataset):
    y_true = []
    y_pred = []

    for item in dataset:
        context = item['context']
        question = item['question']
        choices = item['choices']
        correct_answer = item['correct_choice']

        scores = []
        for choice in choices:
            input_text = f"{context} {question} {choice}"
            inputs = tokenizer(input_text, return_tensors="pt").to('cuda')
            with torch.no_grad():
                outputs = model(**inputs)
            logits = outputs.logits[:, :-1, :]
            target_ids = inputs['input_ids'][:, 1:]
            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
            loss = loss_fct(logits.permute(0, 2, 1), target_ids)
            scores.append(-loss.sum().item())

        predicted_index = torch.argmax(torch.tensor(scores)).item()
        predicted_choice = ['a', 'b', 'c', 'd'][predicted_index]

        y_true.append(correct_answer)
        y_pred.append(predicted_choice)

    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted', labels=['a', 'b', 'c', 'd'])
    recall = recall_score(y_true, y_pred, average='weighted', labels=['a', 'b', 'c', 'd'])
    f1 = f1_score(y_true, y_pred, average='weighted', labels=['a', 'b', 'c', 'd'])

    return accuracy, precision, recall, f1

# Evaluate the model on LogiQA dataset
accuracy, precision, recall, f1 = evaluate_logiqa_pruned(model, tokenizer, logiqa_data)
print(f"LogiQA Evaluation Metrics:")
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision * 100:.2f}%")
print(f"Recall: {recall * 100:.2f}%")
print(f"F1-Score: {f1 * 100:.2f}%")

# Function to load e-KAR dataset from JSON
def load_ekar_dataset(file_path):
    dataset = []
    with open(file_path, 'r') as f:
        for line in f:
            dataset.append(json.loads(line))
    return dataset

# Load the e-KAR dataset
file_path = '/content/validation_ekar.json'
ekar_data = load_ekar_dataset(file_path)
print(f"Dataset size: {len(ekar_data)}")
print(ekar_data[0])

# Evaluation function for the e-KAR dataset
def evaluate_ekar_model(model, tokenizer, dataset):
    y_true = []
    y_pred = []

    for item in dataset:
        question = item['question']
        choices = item['choices']['text']
        correct_answer = item['answerKey']

        scores = []
        for choice in choices:
            input_text = f"{question} {choice}"
            inputs = tokenizer(input_text, return_tensors="pt").to('cuda')
            with torch.no_grad():
                outputs = model(**inputs)
            logits = outputs.logits[:, :-1, :]
            target_ids = inputs['input_ids'][:, 1:]
            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
            loss = loss_fct(logits.permute(0, 2, 1), target_ids)
            scores.append(-loss.sum().item())

        predicted_index = torch.argmax(torch.tensor(scores)).item()
        y_true.append(correct_answer)
        y_pred.append(['A', 'B', 'C', 'D'][predicted_index])

    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')

    return accuracy, precision, recall, f1

# Evaluate the model on e-KAR dataset
accuracy, precision, recall, f1 = evaluate_ekar_model(model, tokenizer, ekar_data)
print(f"e-KAR Evaluation Metrics:")
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision * 100:.2f}%")
print(f"Recall: {recall * 100:.2f}%")
print(f"F1-Score: {f1 * 100:.2f}%")

# Function to evaluate ReClor dataset
def evaluate_reclor_pruned(model, tokenizer, dataset):
    y_true = []
    y_pred = []

    for item in dataset:
        context = item['context']
        question = item['question']
        choices = item['answers']  # List of possible answers
        correct_answer = item['label']  # The correct answer index (0-3)

        scores = []
        for choice in choices:
            input_text = f"{context} {question} {choice}"
            inputs = tokenizer(input_text, return_tensors="pt").to('cuda')
            with torch.no_grad():
                outputs = model(**inputs)
            logits = outputs.logits[:, :-1, :]
            target_ids = inputs['input_ids'][:, 1:]
            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
            loss = loss_fct(logits.permute(0, 2, 1), target_ids)
            scores.append(-loss.sum().item())

        predicted_index = torch.argmax(torch.tensor(scores)).item()
        y_true.append(correct_answer)
        y_pred.append(predicted_index)

    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')

    return accuracy, precision, recall, f1

# Load the ReClor dataset
with open('/content/validation_reclor.json', 'r') as f:
    reclor_data = json.load(f)

print(f"Validation set size: {len(reclor_data)}")
print(reclor_data[0])  # Display the first sample to ensure it's loaded correctly

# Evaluate the model on the ReClor dataset
accuracy, precision, recall, f1 = evaluate_reclor_pruned(model, tokenizer, reclor_data)
print(f"ReClor Evaluation Metrics:")
print(f"Validation Accuracy: {accuracy * 100:.2f}%")
print(f"Validation Precision: {precision * 100:.2f}%")
print(f"Validation Recall: {recall * 100:.2f}%")
print(f"Validation F1-Score: {f1 * 100:.2f}%")