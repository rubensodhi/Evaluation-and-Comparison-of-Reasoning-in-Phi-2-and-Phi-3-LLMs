# -*- coding: utf-8 -*-
"""commonsenseqa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sHnwquFKNjP-U0aG2VjYw1WguRPvAVpF
"""

# Install necessary libraries
!pip install transformers torch pandas pyarrow datasets



import pandas as pd

# Load the train, validation, and test datasets from Parquet files

validation_data = pd.read_parquet('/content/validation-00000-of-00001.parquet')



print(f"Validation set size: {len(validation_data)}")

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load the model and tokenizer
model_name = 'microsoft/Phi-3-mini-4k-instruct'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')

# Set the pad_token if it doesn't exist
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})

# Ensure the model handles padding correctly
model.config.pad_token_id = tokenizer.pad_token_id

print(f"Original model {model_name} loaded successfully.")

# Example prompt to check if the model is running
prompt = "The capital of France is"

# Tokenize the input
inputs = tokenizer(prompt, return_tensors="pt").to('cuda')

# Generate output
with torch.no_grad():
    outputs = model.generate(inputs['input_ids'], max_length=20)

# Decode the output
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("Generated text:", generated_text)

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import torch

def evaluate_metrics_commonsenseqa(model, tokenizer, dataset):
    y_true = []
    y_pred = []

    for _, item in dataset.iterrows():
        question = item['question']
        choices = item['choices']['text']
        correct_answer = item['answerKey']

        # Skip if the correct answer is empty
        if not correct_answer:
            continue

        scores = []
        for choice in choices:
            # Prepare the input with the question and choice
            input_text = f"{question} {choice}"
            inputs = tokenizer(input_text, return_tensors="pt").to('cuda')

            # Generate the output logits
            with torch.no_grad():
                outputs = model(**inputs)

            # Calculate the score (likelihood) for the choice
            logits = outputs.logits[:, :-1, :]
            target_ids = inputs['input_ids'][:, 1:]
            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
            loss = loss_fct(logits.permute(0, 2, 1), target_ids)
            score = -loss.sum().item()  # Negative log-likelihood as the score
            scores.append(score)

        # Select the choice with the highest score
        predicted_index = torch.argmax(torch.tensor(scores)).item()

        # Append true and predicted labels
        y_true.append(ord(correct_answer) - ord('A'))
        y_pred.append(predicted_index)

    # Calculate metrics
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    accuracy = accuracy_score(y_true, y_pred)

    return accuracy, precision, recall, f1

# Evaluate on the validation set
accuracy, precision, recall, f1 = evaluate_metrics_commonsenseqa(model, tokenizer, validation_data)

print(f"Validation Accuracy: {accuracy * 100:.2f}%")
print(f"Validation Precision: {precision * 100:.2f}%")
print(f"Validation Recall: {recall * 100:.2f}%")
print(f"Validation F1-Score: {f1 * 100:.2f}%")

import torch.nn.functional as F
import math

def calculate_perplexity(model, tokenizer, dataset):
    total_loss = 0.0
    total_tokens = 0

    for _, item in dataset.iterrows():
        question = item['question']
        choices = item['choices']['text']

        # Prepare the input with the question and the correct answer
        input_text = f"{question} {choices[ord(item['answerKey']) - ord('A')]}"  # Using the correct answer as target
        inputs = tokenizer(input_text, return_tensors="pt").to('cuda')

        # Generate the output logits
        with torch.no_grad():
            outputs = model(**inputs)

        logits = outputs.logits[:, :-1, :].contiguous().view(-1, outputs.logits.size(-1))
        target_ids = inputs['input_ids'][:, 1:].contiguous().view(-1)

        # Calculate negative log-likelihood
        loss = F.cross_entropy(logits, target_ids, reduction='sum')
        total_loss += loss.item()
        total_tokens += target_ids.numel()

    # Calculate and return perplexity
    perplexity = math.exp(total_loss / total_tokens)
    return perplexity

# Calculate perplexity on the validation set
validation_perplexity = calculate_perplexity(model, tokenizer, validation_data)
print(f"Validation Perplexity: {validation_perplexity:.2f}")