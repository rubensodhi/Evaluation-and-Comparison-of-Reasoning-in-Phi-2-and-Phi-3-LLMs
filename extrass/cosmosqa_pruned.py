# -*- coding: utf-8 -*-
"""cosmosqa_pruned.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nCi-BPEaDqafmt2URG4YgikctIzN2qA3
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Path to the pruned model
pruned_model_path = '/content/wanda/model/smol'  # Adjust to your pruned model path

# Load the pruned model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(pruned_model_path)
pruned_model = AutoModelForCausalLM.from_pretrained(pruned_model_path).to('cuda')

# Ensure the pruned model handles padding correctly
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})
pruned_model.config.pad_token_id = tokenizer.pad_token_id

print(f"Pruned model loaded successfully from {pruned_model_path}.")

import torch
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

def evaluate_cosmosqa_pruned(pruned_model, tokenizer, dataset):
    y_true = []
    y_pred = []

    for item in dataset:
        question = item['question']
        choices = [item[f'answer{i}'] for i in range(4)]  # CosmosQA has four choices
        correct_answer = item['label']  # CosmosQA has a label field that is an integer (0-3)

        scores = []
        for choice in choices:
            # Prepare the input with the question and choice
            input_text = f"{question} {choice}"
            inputs = tokenizer(input_text, return_tensors="pt").to('cuda')

            # Generate the output logits
            with torch.no_grad():
                outputs = pruned_model(**inputs)

            # Calculate the score (likelihood) for the choice
            logits = outputs.logits[:, :-1, :]
            target_ids = inputs['input_ids'][:, 1:]
            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
            loss = loss_fct(logits.permute(0, 2, 1), target_ids)
            score = -loss.sum().item()  # Negative log-likelihood as the score
            scores.append(score)

        # Select the choice with the highest score
        predicted_index = torch.argmax(torch.tensor(scores)).item()

        # Append true and predicted labels
        y_true.append(correct_answer)
        y_pred.append(predicted_index)

    # Calculate metrics
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')

    return accuracy, precision, recall, f1

# Assuming you already have the validation_data loaded
accuracy, precision, recall, f1 = evaluate_cosmosqa_pruned(pruned_model, tokenizer, validation_data)

print(f"Validation Accuracy (Pruned Model): {accuracy * 100:.2f}%")
print(f"Validation Precision (Pruned Model): {precision * 100:.2f}%")
print(f"Validation Recall (Pruned Model): {recall * 100:.2f}%")
print(f"Validation F1-Score (Pruned Model): {f1 * 100:.2f}%")

from google.colab import drive
drive.mount('/content/drive')