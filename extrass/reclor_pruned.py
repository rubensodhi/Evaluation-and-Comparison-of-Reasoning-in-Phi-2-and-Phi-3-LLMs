# -*- coding: utf-8 -*-
"""ReClor_pruned.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kydEHY8zSVYa0V4Z5xhCYxVmmxGeKN-n
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Path to the pruned model
pruned_model_path = '/content/wanda/model/smol'  # Adjust to your pruned model path

# Load the pruned model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(pruned_model_path)
pruned_model = AutoModelForCausalLM.from_pretrained(pruned_model_path).to('cuda')

# Ensure the pruned model handles padding correctly
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})
pruned_model.config.pad_token_id = tokenizer.pad_token_id

print(f"Pruned model loaded successfully from {pruned_model_path}.")

import json

# Load the validation dataset from the provided JSON file
with open('/mnt/data/val.json', 'r') as f:
    validation_data = json.load(f)

print(f"Validation set size: {len(validation_data)}")
print(validation_data[0])  # Display the first sample to ensure it's loaded correctly

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

def evaluate_reclor_pruned(pruned_model, tokenizer, dataset):
    y_true = []
    y_pred = []

    for item in dataset:
        context = item['context']
        question = item['question']
        choices = item['answers']  # List of possible answers
        correct_answer = item['label']  # The correct answer index (0-3)

        scores = []
        for choice in choices:
            # Prepare the input with the context, question, and choice
            input_text = f"{context} {question} {choice}"
            inputs = tokenizer(input_text, return_tensors="pt").to('cuda')

            # Generate the output logits
            with torch.no_grad():
                outputs = pruned_model(**inputs)

            # Calculate the score (likelihood) for the choice
            logits = outputs.logits[:, :-1, :]
            target_ids = inputs['input_ids'][:, 1:]
            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
            loss = loss_fct(logits.permute(0, 2, 1), target_ids)
            score = -loss.sum().item()  # Negative log-likelihood as the score
            scores.append(score)

        # Select the choice with the highest score
        predicted_index = torch.argmax(torch.tensor(scores)).item()

        # Append true and predicted labels
        y_true.append(correct_answer)
        y_pred.append(predicted_index)

    # Calculate metrics
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')

    return accuracy, precision, recall, f1

# Evaluate on the validation set
accuracy, precision, recall, f1 = evaluate_reclor_pruned(pruned_model, tokenizer, validation_data)

print(f"Validation Accuracy (Pruned Model): {accuracy * 100:.2f}%")
print(f"Validation Precision (Pruned Model): {precision * 100:.2f}%")
print(f"Validation Recall (Pruned Model): {recall * 100:.2f}%")
print(f"Validation F1-Score (Pruned Model): {f1 * 100:.2f}%")

import torch.nn.functional as F
import math

def calculate_perplexity_reclor_pruned(pruned_model, tokenizer, dataset):
    total_loss = 0.0
    total_tokens = 0

    for item in dataset:
        context = item['context']
        question = item['question']
        correct_answer = item['answers'][item["label"]]  # Using the correct answer as target

        # Prepare the input with the context, question, and the correct answer
        input_text = f"{context} {question} {correct_answer}"
        inputs = tokenizer(input_text, return_tensors="pt").to('cuda')

        # Generate the output logits
        with torch.no_grad():
            outputs = pruned_model(**inputs)

        logits = outputs.logits[:, :-1, :].contiguous().view(-1, outputs.logits.size(-1))
        target_ids = inputs['input_ids'][:, 1:].contiguous().view(-1)

        # Calculate negative log-likelihood
        loss = F.cross_entropy(logits, target_ids, reduction='sum')
        total_loss += loss.item()
        total_tokens += target_ids.numel()

    # Calculate and return perplexity
    perplexity = math.exp(total_loss / total_tokens)
    return perplexity

# Calculate perplexity on the validation set for the pruned model
validation_perplexity_pruned = calculate_perplexity_reclor_pruned(pruned_model, tokenizer, validation_data)
print(f"Validation Perplexity (Pruned Model): {validation_perplexity_pruned:.2f}")

